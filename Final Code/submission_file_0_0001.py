# -*- coding: utf-8 -*-
"""Submission File 0.0001.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XhzZAy_vhbtoXNpen86wQDO1MTiziDYU
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import imutils
import torch
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
import cv2
from google.colab import drive
import os
from io import BytesIO
from PIL import Image
import IPython.display as display

"""**Certain sections for the OpenCV code were adpated from the following resource:** https://vinodpatildev.medium.com/deep-learning-cnn-model-to-auto-detect-vehicles-number-plate-using-python-and-api-195a773a90e4

**Section 1: License Plate Detection & Character Segmentation**
"""

# Mount Google Drive
drive.mount('/content/drive')


dataset_path = '/content/drive/My Drive/worked_cars/'


segmented_characters = []


for folder_name in os.listdir(dataset_path):# iterating through each folder (class) in the dataset
    folder_path = os.path.join(dataset_path, folder_name)


    current_plate_characters = []


    for img_name in os.listdir(folder_path):
        img_path = os.path.join(folder_path, img_name)
        img = cv2.imread(img_path)


        if img is None:
            print(f"Image not loaded: {img_path}")
            continue
        #the following few sections incorporated ideas and methods from this resource: https://vinodpatildev.medium.com/deep-learning-cnn-model-to-auto-detect-vehicles-number-plate-using-python-and-api-195a773a90e4

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # converting the image to grayscale


        gray = cv2.bilateralFilter(gray, 11, 17, 17) # using bilateral filter to reduce noise


        edges = cv2.Canny(gray, 30, 200) # edge detection

        # Find contours based on edges
        contours = cv2.findContours(edges.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        contours = imutils.grab_contours(contours)
        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:10]

        license_plate_contour = None

        # Interate through contours to find the one that corresponds to the license plate
        for c in contours:
            peri = cv2.arcLength(c, True)
            approx = cv2.approxPolyDP(c, 0.018 * peri, True)
            if len(approx) == 4:
                license_plate_contour = approx
                break

        # If the license plate contour is found, crop the license plate from the image
        if license_plate_contour is not None:
            pts = license_plate_contour.reshape(4, 2)
            rect = np.zeros((4, 2), dtype="float32")

            s = pts.sum(axis=1)

            #identifying the corners based on the sum
            rect[0] = pts[np.argmin(s)] #returns index of the smallest value of 's'
            rect[2] = pts[np.argmax(s)] #returns index of the biggest value of 's'

            diff = np.diff(pts, axis=1) #calculate the difference between coordinates

            #the top-right and bottom-left corners).
            rect[1] = pts[np.argmin(diff)]
            rect[3] = pts[np.argmax(diff)]

            (tl, tr, br, bl) = rect

            # Finding the width of the rectangle to make sure its the largest dimension is being used
            widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2)) #distance between the bottom-left (bl) and bottom-right (br) corners using the Euclidean distance formula
            widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2)) #computed similarly as above
            maxWidth = max(int(widthA), int(widthB))

            #compute the height of the rectangle
            heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))
            heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))
            maxHeight = max(int(heightA), int(heightB))

            # Set up the destination points for the perspective transform
            dst = np.array([
                [0, 0],
                [maxWidth - 1, 0],
                [maxWidth - 1, maxHeight - 1],
                [0, maxHeight - 1]], dtype="float32")



            # # Apply the perspective transform - Got perspective transform syntax and application method from https://www.geeksforgeeks.org/perspective-transformation-python-opencv/
            M = cv2.getPerspectiveTransform(rect, dst)
            license_plate = cv2.warpPerspective(img, M, (maxWidth, maxHeight))

            # Convert the cropped license plate to grayscale
            license_plate_gray = cv2.cvtColor(license_plate, cv2.COLOR_BGR2GRAY) # Convert the cropped license plate to grayscale

            # Apply adaptive thresholding to get a binary image - Got adaptive thresholding syntax and method from https://www.geeksforgeeks.org/python-thresholding-techniques-using-opencv-set-2-adaptive-thresholding/
            thresh = cv2.adaptiveThreshold(license_plate_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)

            # Finding contours on cropped image
            char_contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            char_contours = imutils.grab_contours(char_contours)
            char_contours = sorted(char_contours, key=lambda c: cv2.boundingRect(c)[0])


            individual_chars = [] #list to store individual character images

            # Draw rectangles around each detected character and save individual character images
            for contour in char_contours:
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / float(h)
                if w > 10 and h > 10 and 0.1 < aspect_ratio < 1.0:  # Filter out small contours which cannot be a license plate character
                    char_img = license_plate[y:y + h, x:x + w]
                    char_img_pil = Image.fromarray(char_img)


                    buffered = BytesIO()
                    char_img_pil.save(buffered, format="PNG") # save individual character images

                    individual_chars.append(char_img_pil)# store the character image


            # Display the license plate with highlighted characters
            if license_plate_contour is not None:

                for contour in char_contours:
                    x, y, w, h = cv2.boundingRect(contour) # Highlight the characters detected in the image
                    aspect_ratio = w / float(h)
                    if w > 10 and h > 10 and 0.1 < aspect_ratio < 1.0:  # Filter out small contours which cannot be a license plate character
                        cv2.rectangle(license_plate, (x, y), (x + w, y + h), (0, 255, 0), 2)


                license_plate_pil = Image.fromarray(cv2.cvtColor(license_plate, cv2.COLOR_BGR2RGB))
                display.display(license_plate_pil)

            # Store the list of segmented characters for the current license plate
            if individual_chars:
                segmented_characters.append(individual_chars)
                print(f"New license plate begins with label: {folder_name}")

# Display images
for i, plate in enumerate(segmented_characters):
    print(f"License Plate {i + 1} contains {len(plate)} characters:")
    for j, char_img_pil in enumerate(plate):
        print(f"Character {j + 1}")
        display.display(char_img_pil)
    print("New license plate begins")

"""**Section 2: Alpha-Numeric Character Recognition**

Used lab and lecture content
"""

# Define transformations
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.Grayscale(num_output_channels=1),  # If images are RGB, remove this line
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize for grayscale images
])

# Load dataset
dataset = datasets.ImageFolder('/content/drive/My Drive/A-3_and_numbers/', transform=transform)

# Calculate dataset sizes for train, validation, and test splits
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

# Split dataset
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define the CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(128 * 32 * 32, 256)  # Adjusted input size
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, len(dataset.classes))

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = x.view(x.size(0), -1)  # Flatten the output
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

model = SimpleCNN()


'''
Epochs = 15
Batch Size = 64
Learning Rate = 0.0001
Optimizer = Adam
'''

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Lists to store loss and accuracy values
train_losses = []
val_losses = []
val_accuracies = []

# Training loop
num_epochs = 15
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    correct_train = 0
    total_train = 0

    for inputs, labels in train_loader:
        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(inputs)
        loss = criterion(outputs, labels)

        # Backward pass and optimize
        loss.backward()
        optimizer.step()

        train_loss += loss.item() * inputs.size(0)
        _, predicted = torch.max(outputs, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    # Validation
    model.eval()
    val_loss = 0.0
    correct_val = 0
    total_val = 0

    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * inputs.size(0)

            _, predicted = torch.max(outputs, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    val_loss /= len(val_loader.dataset)
    val_accuracy = 100 * correct_val / total_val

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    val_accuracies.append(val_accuracy)

    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')

# Plotting the loss and accuracy curves
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curve')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy Curve')
plt.legend()

plt.show()

# Function to display images and predictions
def imshow(img, title):
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')
    plt.title(title)
    plt.show()

# Predict and display results on all images from the test set
model.eval()
with torch.no_grad():
    for inputs, labels in test_loader:  # Using the test_loader instead of creating a new DataLoader
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)

        #for i in range(inputs.size(0)):
         #   predicted_label = dataset.classes[predicted[i].item()]
          #  imshow(inputs[i], f'Predicted: {predicted_label}')

# Save the model
model_path_2 = '/content/drive/My Drive/sub_file_0.0001.pth'
torch.save(model.state_dict(), model_path_2)
print(f"Model saved to {model_path_2}")

"""This is a repeated segment of the model abover in oder to use the saved model and avoid training."""

# Define transformations
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.Grayscale(num_output_channels=1),  # If images are RGB, remove this line
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))  # Normalize for grayscale images
])

# Load dataset
dataset = datasets.ImageFolder('/content/drive/My Drive/A-3_and_numbers/', transform=transform)

# Calculate dataset sizes for train, validation, and test splits
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

# Split dataset
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

# Define the CNN Model
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()

        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2, 2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2, 2)

        self.fc1 = nn.Linear(128 * 32 * 32, 256)  # Adjusted input size
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 64)
        self.fc4 = nn.Linear(64, len(dataset.classes))

    def forward(self, x):
        x = self.pool1(F.relu(self.bn1(self.conv1(x))))
        x = self.pool2(F.relu(self.bn2(self.conv2(x))))
        x = x.view(x.size(0), -1)  # Flatten the output
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

model = SimpleCNN()

model_path_2 = '/content/drive/My Drive/sub_file_0.0001.pth'
model.load_state_dict(torch.load(model_path_2))
model.eval()

print("Model loaded")

def get_accuracy(model, data_loader):
    """Calculate accuracy of the model on the given data_loader."""
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data in data_loader:
            images, labels = data
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    model.train()
    return correct / total

# Test accuracy of Alpha Numeric Dataset
test_acc = get_accuracy(model, test_loader)
print(test_acc)

"""**Section 3: Testing on Segmented Images**"""

#Installing the python-Levenshtein package and other important libraries
!pip install python-Levenshtein
import os
import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt
import Levenshtein

transform = transforms.Compose([
    transforms.Resize((128, 128)),#images converted to 128x128 pixels
    transforms.Grayscale(num_output_channels=1), #images are converted to greyscale
    transforms.ToTensor(), #convert to pytorch tensor
    transforms.Normalize((0.5,), (0.5,)) #normalization
])

# Function to display images and their predictions side by side
def show_images_with_predictions(images, predictions):
    num_images = len(images)
    fig, axes = plt.subplots(1, num_images, figsize=(15, 5)) #creating subplots for the images

    if num_images == 1:
        axes = [axes]

    for ax, img, pred in zip(axes, images, predictions):
        ax.imshow(img, cmap='gray')
        ax.set_title(pred, fontsize=10)
        ax.axis('off')

    plt.show()

def calculate_char_accuracy(predicted, actual):
    # Find the length of the shortest string
    min_len = min(len(predicted), len(actual)) # the length of the shorter string is determined,so min function

    # Count matching characters in the overlapping part of the strings
    correct_chars = sum(1 for i in range(min_len) if predicted[i] == actual[i]) # similar characters are counted

    # If no characters match, avoid division by zero
    if correct_chars == 0:
        return 0

    # Accuracy based on the overlapping length
    accuracy = (correct_chars / min_len) * 100 #calculating accuracy percentage
    return accuracy


# Levinshtein code is adapted from : https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/
def calculate_similarity(predicted, actual):

    distance = Levenshtein.distance(predicted, actual) #Levenshtein distance is calculated
    max_len = max(len(predicted), len(actual)) # length of the longer string, so max function

    similarity = (1 - distance / max_len) * 100 #a similarity score is calculated
    return similarity


char_accuracy_scores = [] # list to store character accuracy scores


dataset_path = '/content/drive/MyDrive/worked_cars/'
actual_labels = [folder_name for folder_name in os.listdir(dataset_path)] #iterate through and store folder names

# Initializing lists to store predictions and matched license plates
predictions = []
matched_plates = []

used_labels = set()

# Iterate through the segmented characters by license plate
for plate_index, plate in enumerate(segmented_characters):
    license_plate_number = ""  # Initialize an empty string to store the full string license plate number
    images = []
    pred_labels = []

    print(f"Processing License Plate {plate_index + 1}:")

    for char_index, char_img_pil in enumerate(plate):

        input_tensor = transform(char_img_pil).unsqueeze(0)  #apply transformations and batch dimension

        output = model(input_tensor)
        _, predicted_idx = torch.max(output, 1) #get predicted class index
        predicted_label = dataset.classes[predicted_idx.item()]

        license_plate_number += predicted_label

        # Store image and its prediction
        images.append(char_img_pil)
        pred_labels.append(predicted_label)


    show_images_with_predictions(images, pred_labels)


    predictions.append(license_plate_number)


    print(f"Full License Plate Number: {license_plate_number}")
    print("New license plate begins")

    # assigning the closest label, but ensuring that it has not been used to avoid assigning same labels
    closest_label = min(
        (label for label in actual_labels if label not in used_labels),
        key=lambda label: Levenshtein.distance(license_plate_number, label),
        default=None
    )

    if closest_label:
        # mark the closest label as used
        used_labels.add(closest_label)

        # store and calculate the similarity score between the actual truth label and the predicted license plate
        similarity = calculate_similarity(license_plate_number, closest_label)
        char_accuracy_scores.append(similarity)
        print(f"Similarity with closest actual label ({closest_label}): {similarity:.2f}%")
    else:
        print("No available label to match with the predicted license plate.")

# Calculating overall accuracy
correct_predictions = len([plate for plate in predictions if plate in used_labels])
overall_accuracy = correct_predictions / len(actual_labels) * 100

# Calculating average character-by-character accuracy
average_char_accuracy = sum(char_accuracy_scores) / len(char_accuracy_scores) if char_accuracy_scores else 0

print(f"Overall Accuracy of Correct License Plates: {overall_accuracy:.2f}%")
print(f"Average Character-by-Character Accuracy: {average_char_accuracy:.2f}%")